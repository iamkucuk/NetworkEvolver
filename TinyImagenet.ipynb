{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Process of Network Hyper-Parameters\n",
    "## Prepared by Furkan Küçük for DataBoss Analytics Job Application\n",
    "\n",
    "Decision process of a machine learning pipeline can get very trivial. In this notebook, I will try to explain my general approach on hyper-parameter tuning. \n",
    "\n",
    "First of all, hyper-parameter decision process might take some time. This may be due to shortage of resources (like having a low-spec hardware) or time. Especially tasks on computer vision may take some time since datasets of computer vision tasks may be challenging, have clutter, hard to label etc. Besides, since computer vision tasks are relatively more complicated, it may need more complex deep learning architectures.\n",
    "\n",
    "Important note: This task is being done with a relatively low-spec hardware(Google Colab). Hence, one may need to tune hyper-parameters with pre-acquired insights. The experiments will be held with a light network architecture and will have an assumption for found parameters will apply for all conditions. However, despite this is not an accurate assumption since different hyper-parameters may perform better under different conditions, this experiments can provide a good start point for further hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n",
    "from dataset_utils import alternativeSeperation\n",
    "from model_utils import ModelEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beggining any machine learning task, one have to prepare project spesific data. Tiny Imagenet dataset comes with 3 folders; for training, for validation and for testing. Training folder is well structured for PyTorch's prebuilt ImageFolder dataset handler. However, validation folder has all images in one folder, and labels of them in a text file. I implemented 2 alternatives for dealing such a task. \n",
    "- (Alternative) First one of the alternatives, is an extension for PyTorch's \"nn.Dataset\" class, gets images according to text file. This alternative introduces some overhead to system since it labels all images as the training goes. Further explaination can be obtained from class doc.\n",
    "- (Preferred) Second alternative is a function that seperates images among class folders that created correspondingly. With this function, the need of RAM usage for holding label information becomes unnecessary. This function also enables user to use PyTorch's \"ImageFolder\" implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#alternativeSeperation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\nmd\n"
    }
   },
   "source": [
    "Definition: A loss landscape is a multi-dimensional representation of loss functions. For more information please take a look at [1].\n",
    "\n",
    "In computer vision tasks, most of the loss landscapes are not convex. This property of loss landscapes makes it hard to find the global optima. There are 3 main elements that affects the loss landscape severely; loss function, data distribution and model architecture. (Other hyper-parameters also affect loss landscapes) In computer vision, this data distribution can be adjusted slightly to lead the machine learning pipeline to find the global optima. (or at least a good local optima)\n",
    "\n",
    "- First adjustment should be channel normalization. Normalization is an important step to have an easier loss landscape. In machine learning projects, all features may have their own distributions. However, numerically, a distribution significantly different from other ones (e.g. a distribution with much larger samples) may affect loss landscape severely and decreases gradients on a dimension while increases on another one. (In other words, the optimizer may think that a feature is more important than other one) With channel normalization, one may prevent that issue. This process can be done by statistically analyzing datasets channel distribution. \n",
    "\n",
    "- Data augmentation may be nice to have tool for exploration of loss landscape. A dataset can only represent a partition of a real distribution. With data augmentation, one can slightly increase the coverage of that representation and in some ways enhance it. Some of the benefits of data augmentation are:\n",
    "    - May prevent overfitting (in some cases) for classes by changing translation, rotation, angle etc.\n",
    "    - May balance a dataset that is unbalanced so, the bias of model can be decreased.\n",
    "    - Increase of coverage may lead to better generalization.\n",
    "    \n",
    "However, data augmentation is just replicating an image with some distortions and noise. Overdoing may harm generalization.\n",
    "\n",
    "In this part, we will find out which augmentation performs better for our case.\n",
    "\n",
    "[1] https://arxiv.org/pdf/1712.09913.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transformations = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = \"data\"\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          transformations[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32,\n",
    "                             shuffle=True)\n",
    "               for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may create a network architecture to experiment on. As mentioned before, in this notebook, a single simple network will be created to evaluate some hyper-parameters. Due to limited hardware, optimization for some hyper-parameters will be done with this network. This approach is not accurate, as different hyper-parameters may perform better or worse under different architectures. However, this method may, at least, provide a good starting point for further optimization, as this procedure gives a rough idea on how augmentations affect the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet1, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),    \n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 11 * 11, 200)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.feature_extractor(inputs)\n",
    "        return self.fc(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some more hyper-parameters to tune after deciding on network architecture. These hyper-parameters are also tobe tuned. These hyper-parameters are:\n",
    "- Loss function: A loss function has a crucial effect on loss landscape, and should be selected accordingly to the given task. Cross Entropy Loss is good with multi-class classification tasks. Logarithmic property of the function improves \"convexity\" of the loss function. Further information can be found at PyTorch's doc[1].\n",
    "- Optimizer: While the standard optimizer for any given loss function is stochastic gradient descent (SGD), an adaptive optimizer performs better on hyper-parameter tuning task. Adam optimizer is considered as a good adaptive optimizer choice for various tasks. Please take a look at \"notes on optimizers\" part for further understandings on how optimizers can be used.\n",
    "- Scheduler: A scheduler adapts learning rate of the optimizer accordingly. While adaptive optimizers mostly do not need a scheduler, a SGD optimizer with a good scheduler may outperform all other optimizers.\n",
    "\n",
    "The training procedure will be handled by \"Model Engine\" written for the given projects. This class eases the training, validation and prediction process. One can look at documentation of \"ModelEngine\" class for further information.\n",
    "\n",
    "[1] https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notes On Optimizers\n",
    "- While adaptive methods (especially Adam optimizer) are widely used for experimenting and hyper-parameter tuning, a sequence of partial training procedures employing SGD optimizer by watching learning curve carefully and adapting learning rate accordingly among these training procedures performs the best for the final training. Please note that this performance may vary depending on Machine Learning Engineer's insights.\n",
    "\n",
    "- Adam optimizer is very popular as it is good for experimenting and performs on acceptable levels. While Adam performs good on practice, theoretically, Adam optimizer has a calculation error in adaptive weight decay section. Correction of this error will be discussed in this notebook and will be compared with classic Adam optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model1 = ConvNet1()\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=.01, momentum=.9)\n",
    "scheduler1 = SGDR(optimizer1, len(dataloaders[\"train\"]))\n",
    "engine1 = ModelEngine(model1, criterion1, optimizer1, scheduler=scheduler1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "model1 = ConvNet1()\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=.01, momentum=.9)\n",
    "scheduler1 = SGDR(optimizer1, len(dataloaders[\"train\"]))\n",
    "engine1 = ModelEngine(model1, criterion1, optimizer1, scheduler=scheduler1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf7995282c541489be76dfadacd2057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=10.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=1563.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=1563.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=1563.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=1563.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=1563.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4f1d53d8c2469ea957fa8defe9927d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=1563.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "engine1.fit(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SGDR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, t_max, eta_min=1e-6, last_epoch=-1, t_mult=2, decay_mult = .8):\n",
    "        self.decay_mult = decay_mult\n",
    "        self.t_max = t_max\n",
    "        self.t_mult = t_mult\n",
    "        self.restart_point = t_max\n",
    "        self.eta_min = eta_min\n",
    "        self.restarted_at = 0\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def restart(self):\n",
    "        self.restart_point *= self.t_mult\n",
    "        self.restarted_at = self.last_epoch\n",
    "        self.t_max *= self.decay_mult\n",
    "\n",
    "    def cosine(self, base_lr):\n",
    "        return self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * (self.last_epoch - self.restarted_at) / self.restart_point)) / 2\n",
    "\n",
    "    def get_lr(self):\n",
    "        if (self.last_epoch - self.restarted_at) >= self.restart_point:\n",
    "            self.restart()\n",
    "        return [self.cosine(base_lr) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=True):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        padding_size = kernel_size // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=padding_size,\n",
    "                               stride=stride)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=padding_size,\n",
    "                               stride=stride)\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        if batch_norm:\n",
    "            self.batch_norm_layer = nn.BatchNorm2d(out_channels)\n",
    "        self.size_fix = None\n",
    "        if in_channels != out_channels:\n",
    "            self.size_fix = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        residual = inputs\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.batch_norm_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm_layer(x)\n",
    "        if self.size_fix:\n",
    "            residual = self.size_fix(residual)\n",
    "        return self.relu(x + residual)\n",
    "\n",
    "class ResNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet1, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            ResBlock(in_channels=3, out_channels=128, kernel_size=3, stride=1),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "            ResBlock(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            ResBlock(in_channels=256, out_channels=512, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 32 * 32, 200)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.feature_extractor(inputs)\n",
    "        return self.fc(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "model2 = ResNet1()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.AdamW(model2.parameters())#optim.SGD(model2.parameters(), lr=.01, momentum=.9)\n",
    "scheduler2 = SGDR(optimizer2, len(dataloaders[\"train\"]))\n",
    "engine2 = ModelEngine(model2, criterion2, optimizer2)#, scheduler=scheduler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b4c3c3592b4baf9304ffe98bd1dc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=10.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=3125.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=313.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=3125.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=313.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=3125.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=313.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=3125.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=313.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=3125.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=313.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=3125.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=313.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=3125.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=313.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=3125.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iterations', max=313.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'earlyStopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-99ae94911d04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mengine2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\NetworkEvolver\\model_utils.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataloaders, num_epoch, inplace, early_stopping, retain_graph)\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[0mbest_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss diddn't decrease. Early stopping.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'earlyStopping' is not defined"
     ]
    }
   ],
   "source": [
    "engine2.fit(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}